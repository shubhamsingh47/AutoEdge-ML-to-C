import numpy as np
from logger.custom_logger import CustomLogger
from exception.custom_exception import CustomException
from .base import BaseConverter
from sklearn.linear_model import LogisticRegression
from src.utils import is_standard_scaler, is_minmax_scaler

logger = CustomLogger().get_logger(__name__)

class LinearConverter(BaseConverter):
    def __init__(self, model_path: str = None):
        super().__init__(model_path)

    def convert_to_c(self, func_name: str = "predict_model") -> str:
        if self.model is None:
            raise CustomException("Converter has no loaded estimator; call load() first", None)

        coef = np.asarray(self.model.coef_)
        intercept = np.asarray(self.model.intercept_)

        if coef.ndim == 1:
            return self._emit_regression_or_binary(func_name, coef, intercept)
        elif coef.ndim == 2:
            return self._emit_multiclass(func_name, coef, intercept)
        else:
            raise CustomException("Unsupported coef_ shape", None)

    def _emit_regression_or_binary(self, func_name: str, coef: np.ndarray, intercept: np.ndarray) -> str:
        n_features = coef.shape[0]
        has_scaler = self.scaler is not None

        lines = []
        lines.append("// Auto-generated by AutoEdgeML (linear)")
        lines.append("#pragma once")
        lines.append("#include <math.h>")
        lines.append("")
        if has_scaler:
            if is_standard_scaler(self.scaler):
                means = ", ".join(f"{m:.10f}f" for m in self.scaler.mean_)
                scales = ", ".join(f"{s:.10f}f" for s in self.scaler.scale_)
                lines.append(f"static const float SCALER_MEAN[{n_features}] = {{ {means} }};")
                lines.append(f"static const float SCALER_SCALE[{n_features}] = {{ {scales} }};")
            elif is_minmax_scaler(self.scaler):
                mins = ", ".join(f"{m:.10f}f" for m in self.scaler.data_min_)
                maxs = ", ".join(f"{M:.10f}f" for M in self.scaler.data_max_)
                lines.append(f"static const float SCALER_MIN[{n_features}] = {{ {mins} }};")
                lines.append(f"static const float SCALER_MAX[{n_features}] = {{ {maxs} }};")
            else:
                raise CustomException("Unsupported scaler type", None)
            lines.append("")

        weights = ", ".join(f"{w:.10f}f" for w in coef)
        bias = float(intercept.ravel()[0])
        lines.append(f"static const float WEIGHTS[{n_features}] = {{ {weights} }};")
        lines.append(f"static const float BIAS = {bias:.10f}f;")
        lines.append("")

        lines.append(f"static inline float {func_name}(const float *x, int n_features) {{")
        if has_scaler:
            lines.append(f"    float x_scaled[{n_features}];")
            lines.append("    for (int i = 0; i < n_features; ++i) {")
            if is_standard_scaler(self.scaler):
                lines.append("        x_scaled[i] = (x[i] - SCALER_MEAN[i]) / SCALER_SCALE[i];")
            else:
                lines.append("        if (SCALER_MAX[i] - SCALER_MIN[i] == 0.0f) x_scaled[i] = 0.0f; else")
                lines.append("        x_scaled[i] = (x[i] - SCALER_MIN[i]) / (SCALER_MAX[i] - SCALER_MIN[i]);")
            lines.append("    }")
            xref = "x_scaled"
        else:
            xref = "x"

        lines.append("    float s = BIAS;")
        lines.append("    for (int i = 0; i < n_features; ++i) {")
        lines.append(f"        s += WEIGHTS[i] * {xref}[i];")
        lines.append("    }")

        # logistic detection
        if isinstance(self.model, LogisticRegression):
            lines.append("    /* logistic sigmoid: return probability */")
            lines.append("    return 1.0f / (1.0f + expf(-s));")
        else:
            lines.append("    return s;")

        lines.append("}")
        return "\n".join(lines)

    def _emit_multiclass(self, func_name: str, coef: np.ndarray, intercept: np.ndarray) -> str:
        C, F = coef.shape
        has_scaler = self.scaler is not None
        lines = []
        lines.append("// Auto-generated by AutoEdgeML (multiclass logistic)")
        lines.append("#pragma once")
        lines.append("#include <math.h>")
        lines.append("")
        if has_scaler:
            if is_standard_scaler(self.scaler):
                means = ", ".join(f"{m:.10f}f" for m in self.scaler.mean_)
                scales = ", ".join(f"{s:.10f}f" for s in self.scaler.scale_)
                lines.append(f"static const float SCALER_MEAN[{F}] = {{ {means} }};")
                lines.append(f"static const float SCALER_SCALE[{F}] = {{ {scales} }};")
            elif is_minmax_scaler(self.scaler):
                mins = ", ".join(f"{m:.10f}f" for m in self.scaler.data_min_)
                maxs = ", ".join(f"{M:.10f}f" for M in self.scaler.data_max_)
                lines.append(f"static const float SCALER_MIN[{F}] = {{ {mins} }};")
                lines.append(f"static const float SCALER_MAX[{F}] = {{ {maxs} }};")
            lines.append("")

        for c in range(C):
            w = ", ".join(f"{v:.10f}f" for v in coef[c])
            lines.append(f"static const float W_{c}[{F}] = {{ {w} }};")
            lines.append(f"static const float B_{c} = {float(intercept[c]):.10f}f;")
        lines.append("")

        lines.append(f"static inline int {func_name}(const float *x, int n_features) {{")
        if has_scaler:
            lines.append(f"    float x_scaled[{F}];")
            lines.append("    for (int i = 0; i < n_features; ++i) {")
            if is_standard_scaler(self.scaler):
                lines.append("        x_scaled[i] = (x[i] - SCALER_MEAN[i]) / SCALER_SCALE[i];")
            else:
                lines.append("        if (SCALER_MAX[i] - SCALER_MIN[i] == 0.0f) x_scaled[i] = 0.0f; else")
                lines.append("        x_scaled[i] = (x[i] - SCALER_MIN[i]) / (SCALER_MAX[i] - SCALER_MIN[i]);")
            lines.append("    }")
            xref = "x_scaled"
        else:
            xref = "x"

        lines.append(f"    float scores[{C}];")
        for c in range(C):
            lines.append(f"    scores[{c}] = B_{c};")
            lines.append(f"    for (int i = 0; i < n_features; ++i) scores[{c}] += W_{c}[i] * {xref}[i];")

        lines.append("    /* argmax */")
        lines.append("    int best = 0;")
        lines.append("    float best_s = scores[0];")
        lines.append(f"    for (int k = 1; k < {C}; ++k) {{ if (scores[k] > best_s) {{ best_s = scores[k]; best = k; }} }}")
        lines.append("    return best;")
        lines.append("}")
        return "\n".join(lines)
